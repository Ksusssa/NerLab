{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPDmtKz8XXTMFaKPq0oE4kD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ksusssa/NerLab/blob/main/Ner2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!ls \"/content/drive/My Drive/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CR8-9bzxBk2H",
        "outputId": "57a0ac6a-a188-4867-e0e6-5032547f2fa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            " Classroom  'Colab Notebooks'   Klasster\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filenameFlowers = \"/content/drive/My Drive/Colab Notebooks/Ner1/Flowers\"\n",
        "filenameLists = \"/content/drive/My Drive/Colab Notebooks/Ner1/Lists\""
      ],
      "metadata": {
        "id": "9RtVUW3gDJuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image, ImageOps\n",
        "def AddImage(path):\n",
        "  test_img = Image.open(path)\n",
        "\n",
        "  test_img = test_img.resize((8,8))\n",
        "  test_img = test_img.convert('L')\n",
        "  '''plt.imshow(test_img, cmap = 'gray')\n",
        "  plt.show()'''\n",
        "\n",
        "  test_x = np.array(test_img, np.float32)\n",
        "  test_x = test_x.reshape(-1,64)\n",
        "  test_x = test_x / 255\n",
        "  return test_x[0]"
      ],
      "metadata": {
        "id": "_3Oatvt0-wH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def AddFlowers(x, y,folderPath):\n",
        "  for filename in os.listdir(folderPath):\n",
        "    x.append(AddImage(os.path.join(folderPath, filename)))\n",
        "    y.append(1)\n",
        "\n",
        "def AddLists(x, y,folderPath):\n",
        "  for filename in os.listdir(folderPath):\n",
        "    x.append(AddImage(os.path.join(folderPath, filename)))\n",
        "    y.append(0)"
      ],
      "metadata": {
        "id": "hI_eiXoABfsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x= []\n",
        "y =[]\n",
        "\n",
        "AddFlowers(x,y, filenameFlowers)\n",
        "AddLists(x, y, filenameLists)\n",
        "\n",
        "X_test, X_train, Y_test, Y_test = train_test_split(x,y, test_size = 0.2)"
      ],
      "metadata": {
        "id": "on5ba83JFdDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPMwEhNA0Ipk",
        "outputId": "49535295-d05a-4da6-b976-07c3e9667777"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Эпоха 0\n",
            "Эпоха 10\n",
            "Эпоха 20\n",
            "Эпоха 30\n",
            "Эпоха 40\n",
            "Эпоха 50\n",
            "Эпоха 60\n",
            "Эпоха 70\n",
            "Эпоха 80\n",
            "Эпоха 90\n",
            "Эпоха 100\n",
            "Эпоха 110\n",
            "Эпоха 120\n",
            "Эпоха 130\n",
            "Эпоха 140\n",
            "Эпоха 150\n",
            "Эпоха 160\n",
            "Эпоха 170\n",
            "Эпоха 180\n",
            "Эпоха 190\n",
            "[[0.39305042 0.7762468  0.43591444 0.18783244 0.62659915 0.59225635\n",
            "  0.16159848 0.10651219 0.0109754  0.8139461 ]\n",
            " [0.29911369 0.92349968 0.5256635  0.72593851 0.99062764 0.55016249\n",
            "  0.81165391 0.3642478  0.52521142 0.364745  ]\n",
            " [0.26091359 0.77842646 0.98159546 0.97914751 0.51273842 0.0528288\n",
            "  0.85194525 0.65664203 0.80052093 0.49263393]\n",
            " [0.97481874 0.34586602 0.51811504 0.67424602 0.35570146 0.36024131\n",
            "  0.50661967 0.35807222 0.1256158  0.10281322]\n",
            " [0.5796419  0.14052864 0.70690234 0.98236935 0.37105765 0.42277426\n",
            "  0.44505901 0.41015861 0.24495014 0.79741218]\n",
            " [0.64889504 0.17154186 0.31493909 0.71052352 0.75552658 0.67012228\n",
            "  0.64740365 0.10480383 0.14164983 0.20020126]\n",
            " [0.46436812 0.29192803 0.48153907 0.08841013 0.72147561 0.03732736\n",
            "  0.14621658 0.79399232 0.32750897 0.40812336]\n",
            " [0.95962679 0.81640561 0.25257437 0.91031392 0.27621149 0.61466395\n",
            "  0.3247189  0.02836113 0.34466092 0.22508115]\n",
            " [0.44198078 0.27328463 0.51909363 0.07820163 0.9088251  0.99734077\n",
            "  0.80579537 0.73116463 0.39647694 0.83832723]\n",
            " [0.83021828 0.06556887 0.62310523 0.22335621 0.89244421 0.7819388\n",
            "  0.32291032 0.03917034 0.10068844 0.3914084 ]\n",
            " [0.38611559 0.46771942 0.93609583 0.94046281 0.18452508 0.06028152\n",
            "  0.24620663 0.12995875 0.36624403 0.03870104]\n",
            " [0.00863912 0.64341528 0.83078503 0.63460279 0.6500441  0.58085943\n",
            "  0.55822388 0.06855068 0.5021436  0.27752285]\n",
            " [0.66384429 0.41882408 0.13013195 0.67782153 0.65953416 0.75830519\n",
            "  0.78227124 0.20425408 0.58594491 0.68626638]\n",
            " [0.64480414 0.14624739 0.02540085 0.02447021 0.99052054 0.31812815\n",
            "  0.80176071 0.43844166 0.60971446 0.99857789]\n",
            " [0.47742558 0.67882835 0.93794698 0.44234075 0.85142569 0.242694\n",
            "  0.92628321 0.8164432  0.53690719 0.01578393]\n",
            " [0.18155636 0.2799145  0.46288248 0.95851787 0.12612176 0.11891722\n",
            "  0.79951417 0.39885937 0.72306073 0.66917696]\n",
            " [0.52681919 0.74102546 0.04212218 0.58760433 0.69005983 0.23310983\n",
            "  0.08514334 0.67091046 0.06664182 0.02673063]\n",
            " [0.08487528 0.44009045 0.4706026  0.72566447 0.44453391 0.56162355\n",
            "  0.84754815 0.1239325  0.80964746 0.08460139]\n",
            " [0.90522906 0.2681625  0.69933063 0.1528618  0.1658605  0.29685245\n",
            "  0.61048102 0.67084891 0.45961746 0.86201541]\n",
            " [0.36321049 0.4434445  0.86687469 0.44954469 0.77634777 0.72652058\n",
            "  0.6815437  0.97618909 0.94268537 0.35692125]\n",
            " [0.34168839 0.8705007  0.35624094 0.712178   0.1603145  0.7030587\n",
            "  0.23272667 0.96404957 0.12003246 0.92621888]\n",
            " [0.78904683 0.53743093 0.4991033  0.62699729 0.2728621  0.52553083\n",
            "  0.16841581 0.60659133 0.55237434 0.03521996]\n",
            " [0.82467684 0.35057083 0.59741854 0.65547959 0.37853971 0.17180016\n",
            "  0.86965729 0.45912144 0.68139463 0.1518132 ]\n",
            " [0.88679918 0.47280143 0.08206775 0.28570544 0.03245912 0.04420663\n",
            "  0.83201069 0.18913625 0.61904845 0.21386544]\n",
            " [0.10221874 0.3444249  0.11002181 0.78594362 0.41540229 0.14003099\n",
            "  0.20911455 0.19171012 0.65215828 0.39331684]\n",
            " [0.94764854 0.16276311 0.0496337  0.7143618  0.90430532 0.14586303\n",
            "  0.3911364  0.25508915 0.63331658 0.76335016]\n",
            " [0.52023026 0.12674551 0.66788922 0.8159836  0.74450328 0.99507692\n",
            "  0.37196415 0.9943501  0.21526395 0.90961997]\n",
            " [0.22121948 0.31228829 0.47395403 0.05502463 0.4424249  0.54635333\n",
            "  0.38237884 0.32920672 0.34544646 0.5053178 ]\n",
            " [0.16776057 0.98731745 0.54537023 0.80840644 0.95839003 0.67344286\n",
            "  0.36662789 0.41395563 0.64791949 0.87424788]\n",
            " [0.57535646 0.47143579 0.54009251 0.73587621 0.54134867 0.24798733\n",
            "  0.77673312 0.95692762 0.37283791 0.91239051]\n",
            " [0.91401817 0.8875832  0.2192848  0.00514792 0.12845827 0.30131642\n",
            "  0.10230814 0.56529814 0.9302309  0.07348979]\n",
            " [0.66118761 0.5618358  0.39256468 0.27747532 0.68000877 0.30987073\n",
            "  0.19871965 0.20345321 0.81634333 0.39594925]\n",
            " [0.7799211  0.67151551 0.69476501 0.78911329 0.06669357 0.48961127\n",
            "  0.15984327 0.75710954 0.60581569 0.68961709]\n",
            " [0.1426037  0.32437075 0.6847553  0.34737902 0.34373612 0.33695047\n",
            "  0.24358403 0.04662999 0.27516879 0.41995945]\n",
            " [0.84060373 0.69973576 0.6772576  0.61647284 0.20673403 0.93244218\n",
            "  0.85873876 0.18085917 0.96499631 0.06179327]\n",
            " [0.90306699 0.07065671 0.96711924 0.17807531 0.6249016  0.31349405\n",
            "  0.86225518 0.31532435 0.95872218 0.0549306 ]\n",
            " [0.13523382 0.08413471 0.16501959 0.03917415 0.82312605 0.50628734\n",
            "  0.8495431  0.78291097 0.65011539 0.25326252]\n",
            " [0.51606127 0.39473432 0.31126221 0.78667572 0.90647845 0.63707732\n",
            "  0.27843331 0.32095132 0.1642388  0.74825874]\n",
            " [0.80670004 0.29329675 0.80110698 0.18845704 0.05076462 0.36209477\n",
            "  0.14822851 0.89655735 0.47081556 0.53820063]\n",
            " [0.35211517 0.92278103 0.93894567 0.022477   0.48804907 0.28412245\n",
            "  0.05021446 0.34986617 0.13433887 0.13497099]\n",
            " [0.9837133  0.9615394  0.365475   0.33179668 0.67555864 0.74746599\n",
            "  0.23578299 0.38157378 0.68250874 0.75172256]\n",
            " [0.62232456 0.0656068  0.2284105  0.87467659 0.13904182 0.8861118\n",
            "  0.88714108 0.57672665 0.73836716 0.18544408]\n",
            " [0.1767549  0.90186819 0.83716097 0.20027341 0.00728365 0.39263455\n",
            "  0.73410079 0.22112097 0.14911855 0.59085913]\n",
            " [0.34769706 0.17815623 0.95367437 0.49704601 0.06134474 0.88153169\n",
            "  0.33417948 0.42544449 0.00725327 0.00866724]\n",
            " [0.04736648 0.74465392 0.03134744 0.17365678 0.83003259 0.63413489\n",
            "  0.04323314 0.30050937 0.65354037 0.54303877]\n",
            " [0.22922695 0.86181994 0.45040929 0.24492339 0.94670338 0.52675275\n",
            "  0.64785562 0.36317865 0.430012   0.02954543]\n",
            " [0.96697922 0.95885153 0.5897923  0.48280402 0.8539538  0.70637611\n",
            "  0.87120385 0.66883419 0.32438269 0.9094283 ]\n",
            " [0.97655105 0.62644466 0.02111325 0.10262655 0.01077824 0.091422\n",
            "  0.29722291 0.7360735  0.12993628 0.12359227]\n",
            " [0.91408313 0.22635263 0.71207365 0.03437435 0.64931642 0.32979688\n",
            "  0.33522949 0.20272056 0.44869791 0.74281523]\n",
            " [0.3200129  0.90231718 0.59092233 0.1675827  0.94446636 0.76763949\n",
            "  0.15438631 0.1770572  0.89484785 0.7849771 ]\n",
            " [0.89142415 0.41076622 0.42723143 0.66650377 0.06728249 0.72656088\n",
            "  0.64217989 0.59988488 0.96493015 0.66343245]\n",
            " [0.16440351 0.59086826 0.07813509 0.60212411 0.27694792 0.63229991\n",
            "  0.98278881 0.0187915  0.3505903  0.48361069]\n",
            " [0.43057408 0.93023413 0.75337765 0.17273919 0.57725713 0.36054215\n",
            "  0.8675513  0.92265901 0.00445331 0.88780155]\n",
            " [0.0798739  0.68299734 0.2771379  0.5760363  0.96696245 0.9224793\n",
            "  0.98495502 0.09274994 0.08141757 0.23307693]\n",
            " [0.41222194 0.07797245 0.36028273 0.27489424 0.64135229 0.87563919\n",
            "  0.35424087 0.72113485 0.85000491 0.01572551]\n",
            " [0.63098742 0.92765364 0.96698213 0.14651062 0.09091308 0.90913967\n",
            "  0.27079752 0.32263126 0.43532082 0.12730379]\n",
            " [0.53844756 0.05131956 0.68052608 0.95239143 0.43447798 0.8324553\n",
            "  0.46757892 0.99455002 0.49633089 0.08063426]\n",
            " [0.54213925 0.08335676 0.24186498 0.45985858 0.81631167 0.99844976\n",
            "  0.47642544 0.02927954 0.30683521 0.01139028]\n",
            " [0.06756128 0.91795221 0.56345273 0.75002024 0.00301981 0.08924493\n",
            "  0.99016937 0.6782026  0.20424983 0.53839863]\n",
            " [0.65153831 0.16586425 0.42619394 0.95153739 0.2842155  0.17266439\n",
            "  0.33782039 0.03932858 0.52955875 0.51370098]\n",
            " [0.19670408 0.25761595 0.89740486 0.39650252 0.4754018  0.58205866\n",
            "  0.45503112 0.56944377 0.4286409  0.42331365]\n",
            " [0.81934418 0.80321541 0.99538589 0.0422621  0.19719014 0.2164597\n",
            "  0.1251955  0.58098729 0.48455154 0.00705655]\n",
            " [0.67797751 0.54852971 0.36326028 0.87923572 0.27755514 0.86780114\n",
            "  0.94625453 0.99731076 0.54029357 0.37287013]\n",
            " [0.94673215 0.06461791 0.55465124 0.70272599 0.49972562 0.81251203\n",
            "  0.24487862 0.17977496 0.08057094 0.56934741]]\n",
            "[[0.47852977 0.50146189 0.40168746 0.37830405 0.69307982 0.34751021\n",
            "  0.51122543 0.03905696 0.13243344 0.03425248]\n",
            " [0.23200056 0.83136789 0.11001165 0.33740295 0.64995827 0.98141235\n",
            "  0.06867504 0.34766708 0.47055467 0.56227828]\n",
            " [0.18291578 0.58826026 0.19687374 0.86323882 0.28609091 0.73912145\n",
            "  0.6505981  0.07223617 0.69790457 0.48560296]\n",
            " [0.93619731 0.25741779 0.24628992 0.409739   0.11813616 0.48326843\n",
            "  0.83700544 0.95836468 0.04240538 0.94224903]\n",
            " [0.39950181 0.13255016 0.91059178 0.58643619 0.36502334 0.76743135\n",
            "  0.78861126 0.3579862  0.84059627 0.91404282]\n",
            " [0.39373969 0.37197809 0.69797054 0.3523502  0.39165437 0.08569068\n",
            "  0.97615451 0.43260083 0.43422303 0.63672098]\n",
            " [0.28717482 0.3144558  0.81666064 0.33396528 0.09438454 0.94050076\n",
            "  0.45098113 0.7189966  0.04013098 0.45174373]\n",
            " [0.07430947 0.77157824 0.44009387 0.56476488 0.89321912 0.23202365\n",
            "  0.17180859 0.84738078 0.2058703  0.63238737]\n",
            " [0.9603669  0.34790224 0.75189592 0.50027027 0.76446137 0.13196042\n",
            "  0.79492241 0.10540672 0.68782027 0.96985213]\n",
            " [0.62467716 0.23156996 0.36757783 0.70577609 0.54692046 0.14555049\n",
            "  0.86338394 0.57208187 0.1905566  0.07975044]]\n"
          ]
        }
      ],
      "source": [
        "#Нейронная сеть, распознование цветков и листков\n",
        "#Реализация при помощи сети Хебба\n",
        "#На входном слое 36 нейронов\n",
        "#На скрытом слое 10 нейронов\n",
        "#На выходе 10 нейронов\n",
        "import numpy as np\n",
        "\n",
        "class NeuralNet:\n",
        "    '''\n",
        "    Нейросеть, распознование цветков и листков\n",
        "    Сеть Хебба\n",
        "    На входе 64 нейронов, изображение 8x8\n",
        "    На скрытом слое 10 нейронов\n",
        "    На выходном слое 10 нейронов\n",
        "    '''\n",
        "    def __init__(self): #Инициализация компонентов нейросети\n",
        "        self.weights1 = np.random.rand(64, 10)\n",
        "        self.weights2 = np.random.rand(10, 10)\n",
        "\n",
        "    def get_weights(self):\n",
        "        return self.weights1, self.weights2\n",
        "\n",
        "    def threshold_function(self, x): #Пороговая функция активации\n",
        "        return 1 if x >= 0 else 0\n",
        "\n",
        "    def feedforward(self, x): #Метод прямого распространения\n",
        "        h = np.zeros(10) #Внутренний слой\n",
        "        for i in range(10):\n",
        "            summator = 0\n",
        "            for j in range(64):\n",
        "                summator += x[j] * self.weights1[j, i]\n",
        "            h[i] = self.threshold_function(summator)\n",
        "\n",
        "        o = np.zeros(10) #Выходной слой\n",
        "        for i in range(10):\n",
        "            summator = 0\n",
        "            for j in range(10):\n",
        "                summator += h[j] * self.weights2[j, i]\n",
        "            o[i] = self.threshold_function(summator)\n",
        "        try:\n",
        "            return list(o).index(1)\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "    def train(self, epochs, X, Y, learning_rate): #Тренировка нейросети по правилу Хебба\n",
        "        for epoch in range(epochs): # Обучаем по эпохам\n",
        "            for sample_index in range(len(X)):\n",
        "                x = X[sample_index] # Одна выборка из множества\n",
        "                y = Y[sample_index]\n",
        "\n",
        "                h = np.zeros(10) #Внутренний слой\n",
        "                for i in range(10):\n",
        "                    summator = 0\n",
        "                    for j in range(64):\n",
        "                        summator += x[j] * self.weights1[j, i]\n",
        "                    h[i] = self.threshold_function(summator)\n",
        "\n",
        "                o = np.zeros(10) #Выходной слой\n",
        "                for i in range(10):\n",
        "                    summator = 0\n",
        "                    for j in range(10):\n",
        "                        summator += x[j] * self.weights2[j, i]\n",
        "                    o[i] = self.threshold_function(summator)\n",
        "\n",
        "                #Пересчитываю связи между выходными нейронами и скрытыми слоями\n",
        "                for i in range(10):\n",
        "                    for j in range(10):\n",
        "                        self.weights2[j, i] += learning_rate * (y - o[i]) * h[j]\n",
        "                \n",
        "                #Пересчитываю связи между входом и скрытым слоем\n",
        "                for i in range(10):\n",
        "                    for j in range(64):\n",
        "                        self.weights1[j, i] += learning_rate * (y - o[i]) * x[j]\n",
        "                        \n",
        "            \n",
        "            if epoch % 10 == 0:\n",
        "                print(f\"Эпоха {epoch}\")    \n",
        "\n",
        "nn = NeuralNet()\n",
        "nn.train(200, X_train, y, 0.2)\n",
        "print(*nn.get_weights(),sep='\\n')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class NeuralNet:\n",
        "    def __init__(self, learning_rate = 0.1):\n",
        "        self.weights_x_h = np.random.normal(size=(64, 10))\n",
        "        self.weights_h_o = np.random.normal(size=(10, 10))\n",
        "        self.sigmoid_mapper = np.vectorize(self.sigmoid) # Функция, котороя позволяет применить ко всему вектору функцию sigmoid\n",
        "        self.learning_rate = np.array([learning_rate]) # Конвертируем rate в np.array что бы можно было с работать с numpy\n",
        "\n",
        "    def sigmoid(self, x): # Функция нормализации сигмоид\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def MSE(self, y_pred, y_true): #Функция потерь\n",
        "        return np.mean((y_pred - y_true) ** 2)\n",
        "\n",
        "    def predict(self, input): # Предсказание (input == x)\n",
        "        h_summator = np.dot(input, self.weights_x_h) # Суммирует произведения входящих нейронов на веса, ведущие к нейрону скрытого слоя\n",
        "        h = self.sigmoid_mapper(h_summator) # Вектор значений нейронов скрытого слоя после нормализации\n",
        "        o_summator = np.dot(h, self.weights_h_o)\n",
        "        o = self.sigmoid_mapper(o_summator) # Вектор значений выходных нейронов\n",
        "        result = \"\\nПроверка\\n\"\n",
        "        for i in range(10):\n",
        "            if o[i] > 0.05:\n",
        "                result += f\"Это цветок\\n\"\n",
        "            else:\n",
        "                result += f\"Это лист\\n\"\n",
        "        return result + \"\\n\"\n",
        "\n",
        "    def train(self, X_train, y_true, epochs): # Обучение сети методом обратного распространения ошибки (input == x, expected == y)\n",
        "        for epoch in range(epochs):\n",
        "            for input, expected in zip(X_train, y_true):\n",
        "                input = np.transpose(input) # Превращаем векторы в столбцы-векторы\n",
        "                expected = np.transpose(expected)\n",
        "                # Проходим вперед и ищем вектор o (1x10) - предсказания\n",
        "                h_summator = np.dot(input, self.weights_x_h)\n",
        "                h = self.sigmoid_mapper(h_summator)\n",
        "                o_summator = np.dot(h, self.weights_h_o)\n",
        "                o = self.sigmoid_mapper(o_summator)\n",
        "                # Ищем ошибку и распространяем её назад, находя изменения весов\n",
        "                # Ищем ошибку со скрытого слоя на выходной и дельту w от o\n",
        "                error_o = expected - o\n",
        "                grad_o = o * (1 - o)\n",
        "                delta_w_o = error_o * grad_o\n",
        "                # Ищем ошибку со входного слоя на скрытый и дельту w от h\n",
        "                error_h = np.dot(delta_w_o, self.weights_h_o.T)\n",
        "                grad_h = h * (1 - h)\n",
        "                delta_w_h = error_h * grad_h\n",
        "                # Изменяем веса\n",
        "                self.weights_x_h += np.dot(input.reshape(64, 1), delta_w_h.reshape(1, 10))\n",
        "                self.weights_h_o += np.dot(h.reshape(10, 1), delta_w_o.reshape(1, 10))\n",
        "            if epoch % 10 == 0:\n",
        "                print(f\"Эпоха {epoch}\")\n",
        "\n",
        "nn = NeuralNet(0.1)\n",
        "nn.train(X_train, y, 1000)\n",
        "\n",
        "#Проверка на тестовой выборке\n",
        "print(nn.predict(X_test[0]))"
      ],
      "metadata": {
        "id": "JS6al-im0Y-i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8362487-d8b5-4dd8-e7fb-5179f7af41d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Эпоха 0\n",
            "Эпоха 10\n",
            "Эпоха 20\n",
            "Эпоха 30\n",
            "Эпоха 40\n",
            "Эпоха 50\n",
            "Эпоха 60\n",
            "Эпоха 70\n",
            "Эпоха 80\n",
            "Эпоха 90\n",
            "Эпоха 100\n",
            "Эпоха 110\n",
            "Эпоха 120\n",
            "Эпоха 130\n",
            "Эпоха 140\n",
            "Эпоха 150\n",
            "Эпоха 160\n",
            "Эпоха 170\n",
            "Эпоха 180\n",
            "Эпоха 190\n",
            "Эпоха 200\n",
            "Эпоха 210\n",
            "Эпоха 220\n",
            "Эпоха 230\n",
            "Эпоха 240\n",
            "Эпоха 250\n",
            "Эпоха 260\n",
            "Эпоха 270\n",
            "Эпоха 280\n",
            "Эпоха 290\n",
            "Эпоха 300\n",
            "Эпоха 310\n",
            "Эпоха 320\n",
            "Эпоха 330\n",
            "Эпоха 340\n",
            "Эпоха 350\n",
            "Эпоха 360\n",
            "Эпоха 370\n",
            "Эпоха 380\n",
            "Эпоха 390\n",
            "Эпоха 400\n",
            "Эпоха 410\n",
            "Эпоха 420\n",
            "Эпоха 430\n",
            "Эпоха 440\n",
            "Эпоха 450\n",
            "Эпоха 460\n",
            "Эпоха 470\n",
            "Эпоха 480\n",
            "Эпоха 490\n",
            "Эпоха 500\n",
            "Эпоха 510\n",
            "Эпоха 520\n",
            "Эпоха 530\n",
            "Эпоха 540\n",
            "Эпоха 550\n",
            "Эпоха 560\n",
            "Эпоха 570\n",
            "Эпоха 580\n",
            "Эпоха 590\n",
            "Эпоха 600\n",
            "Эпоха 610\n",
            "Эпоха 620\n",
            "Эпоха 630\n",
            "Эпоха 640\n",
            "Эпоха 650\n",
            "Эпоха 660\n",
            "Эпоха 670\n",
            "Эпоха 680\n",
            "Эпоха 690\n",
            "Эпоха 700\n",
            "Эпоха 710\n",
            "Эпоха 720\n",
            "Эпоха 730\n",
            "Эпоха 740\n",
            "Эпоха 750\n",
            "Эпоха 760\n",
            "Эпоха 770\n",
            "Эпоха 780\n",
            "Эпоха 790\n",
            "Эпоха 800\n",
            "Эпоха 810\n",
            "Эпоха 820\n",
            "Эпоха 830\n",
            "Эпоха 840\n",
            "Эпоха 850\n",
            "Эпоха 860\n",
            "Эпоха 870\n",
            "Эпоха 880\n",
            "Эпоха 890\n",
            "Эпоха 900\n",
            "Эпоха 910\n",
            "Эпоха 920\n",
            "Эпоха 930\n",
            "Эпоха 940\n",
            "Эпоха 950\n",
            "Эпоха 960\n",
            "Эпоха 970\n",
            "Эпоха 980\n",
            "Эпоха 990\n",
            "\n",
            "Проверка\n",
            "Это цветок\n",
            "Это цветок\n",
            "Это цветок\n",
            "Это цветок\n",
            "Это цветок\n",
            "Это цветок\n",
            "Это цветок\n",
            "Это цветок\n",
            "Это цветок\n",
            "Это цветок\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_test_train():\n",
        "    '''Высчитывает разницу между тренировочной и тестовой выборкой'''\n",
        "    differense = 0\n",
        "    all_elements = 0\n",
        "    for test_number, train_number in zip(X_test, X_train):\n",
        "        for test_el, train_el in zip(test_number, train_number):\n",
        "            all_elements += 1\n",
        "            if test_el != train_el:\n",
        "                differense += 1\n",
        "    return f\"Разница = {round(differense / all_elements * 100, 2)}%\"\n",
        "\n",
        "print(compare_test_train())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scGS01HnVWQX",
        "outputId": "9b575964-1f59-4a6f-9ea2-4e6295bed0a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Разница = 31.77%\n"
          ]
        }
      ]
    }
  ]
}