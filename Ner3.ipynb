{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPA5sv7WTPbfj1cWWwBsf+t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ksusssa/NerLab/blob/main/Ner3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mC2iVyX_5hl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "cup1 =   [0,1,0,0,0,\n",
        "           1,0,1,1,0,\n",
        "           1,1,1,0,1,\n",
        "           1,0,1,1,0,\n",
        "           1,1,1,0,0]\n",
        "\n",
        "cup2 =   [0,1,0,0,0,\n",
        "           1,0,1,1,0,\n",
        "           1,1,1,0,1,\n",
        "           1,0,1,1,0,\n",
        "           1,1,1,0,0]\n",
        "\n",
        "cup3 =   [0,1,0,0,0,\n",
        "           1,0,1,1,0,\n",
        "           1,1,1,0,1,\n",
        "           1,0,1,1,0,\n",
        "           1,1,1,0,0]\n",
        "\n",
        "cup4 =   [0,1,0,0,0,\n",
        "           1,0,1,1,0,\n",
        "           1,1,1,0,1,\n",
        "           1,0,1,1,0,\n",
        "           1,1,1,0,0]\n",
        "\n",
        "cup5 =   [0,1,0,0,0,\n",
        "           1,0,1,1,0,\n",
        "           1,1,1,0,1,\n",
        "           1,0,1,1,0,\n",
        "           1,1,1,0,0]\n",
        "\n",
        "butle1 =  [0,1,1,0,0,\n",
        "            0,1,1,0,0,\n",
        "            1,0,0,1,0,\n",
        "            1,0,0,1,0,\n",
        "            1,1,1,1,0]\n",
        "\n",
        "butle2 =  [0,1,1,0,0,\n",
        "            0,1,1,0,0,\n",
        "            1,0,0,1,0,\n",
        "            1,0,0,1,0,\n",
        "            1,1,1,1,0]\n",
        "\n",
        "butle3 =  [0,1,1,0,0,\n",
        "            0,1,1,0,0,\n",
        "            1,0,0,1,0,\n",
        "            1,0,0,1,0,\n",
        "            1,1,1,1,0]\n",
        "\n",
        "butle4 =  [0,1,1,0,0,\n",
        "            0,1,1,0,0,\n",
        "            1,0,0,1,0,\n",
        "            1,0,0,1,0,\n",
        "            1,1,1,1,0]\n",
        "\n",
        "butle5 =  [0,1,1,0,0,\n",
        "            0,1,1,0,0,\n",
        "            1,0,0,1,0,\n",
        "            1,0,0,1,0,\n",
        "            1,1,1,1,0]\n",
        "\n",
        "def get_cup():\n",
        "    return np.array([cup1, cup2, cup3, cup4, cup5])\n",
        "\n",
        "def get_butle():\n",
        "    return np.array([butle1, butle2, butle3, butle4, butle5])\n",
        "\n",
        "def get_conv_numbers(number):\n",
        "    if number == 1:\n",
        "        return [[np.sum(row) for row in num] for num in get_butle().reshape(5, 5, 5)]\n",
        "    elif number == 0:\n",
        "        return [[np.sum(row) for row in num] for num in get_cup().reshape(5, 5, 5)]\n",
        "    else:\n",
        "        raise Exception(\"Нет такой цифры, введите либо чашку либо бутылку\")\n",
        "\n",
        "def get_conv_number(number):\n",
        "    return np.array([np.sum(row) for row in number.reshape(5, 5)])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "def add_bias_feature(a):\n",
        "    a_extended = np.zeros((a.shape[0],a.shape[1]+1))\n",
        "    a_extended[:,:-1] = a\n",
        "    a_extended[:,-1] = int(1)  \n",
        "    return a_extended\n",
        "\n",
        "class SVM(object):\n",
        "    def __init__(self, etha=0.01, alpha=0.1, epochs=200):\n",
        "        self._epochs = epochs\n",
        "        self._etha = etha\n",
        "        self._alpha = alpha\n",
        "        self._w = None # Веса модели\n",
        "        self.history_w = [] # История изменения весов для показа\n",
        "        self.train_errors = None # Ошибки обучения модели\n",
        "        self.val_errors = None # Ошибки проверки качества модели\n",
        "        self.train_loss = None # Значение функции потерь модели при обучении\n",
        "        self.val_loss = None # Значение функции потерь модели при проверке\n",
        "\n",
        "    def fit(self, X_train, Y_train, X_val, Y_val, verbose=False): #arrays: X; Y =-1,1\n",
        "        '''\n",
        "        Метод обучения модели, значения массивов y принимают значения либо 1 либо -1\n",
        "        1 - если на выходе должна быть еденица, -1 если ноль \n",
        "        '''\n",
        "        if len(set(Y_train)) != 2 or len(set(Y_val)) != 2: # Проверка, что бы количество классов было равно 2 (бин классификация)\n",
        "            raise ValueError(\"Number of classes in Y is not equal 2!\")\n",
        "\n",
        "        X_train = add_bias_feature(X_train) # Добавление признака смещения в каждый столбец\n",
        "        X_val = add_bias_feature(X_val)T\n",
        "        self._w = np.random.normal(loc=0, scale=0.05, size=X_train.shape[1]) # Инициализируем веса случайными значениями\n",
        "        self.history_w.append(self._w) # Записываем начальные веса в историю весов\n",
        "        # Инициализируем списки\n",
        "        train_errors = []\n",
        "        val_errors = []\n",
        "        train_loss_epoch = []\n",
        "        val_loss_epoch = []\n",
        "        # Начало тренировки модели\n",
        "        for epoch in range(self._epochs):\n",
        "            tr_err = 0\n",
        "            val_err = 0\n",
        "            tr_loss = 0\n",
        "            val_loss = 0\n",
        "            for i,x in enumerate(X_train): # Индексируем элементы обучающей выборки\n",
        "                margin = Y_train[i]*np.dot(self._w,X_train[i]) # Находим значение отступа модели\n",
        "                if margin >= 1: # классифицируем верно, если отступ элемента больше радиуса полосы модели\n",
        "                    self._w -= self._etha*self._alpha*self._w/self._epochs # Изменяем веса пропорционально эпохе обучения\n",
        "                    tr_loss += self.soft_margin_loss(X_train[i],Y_train[i]) # Увеличения значерия потерь\n",
        "                else: # классифицируем неверно или попадаем на полосу разделения при 0<m<1\n",
        "                    self._w += self._etha*(Y_train[i]*X_train[i] - self._alpha*self._w/self._epochs)\n",
        "                    tr_err += 1\n",
        "                    tr_loss += self.soft_margin_loss(X_train[i],Y_train[i])\n",
        "                self.history_w.append(self._w)\n",
        "            for i,x in enumerate(X_val): # Цикл проверки качества модели\n",
        "                val_loss += self.soft_margin_loss(X_val[i], Y_val[i])\n",
        "                val_err += (Y_val[i]*np.dot(self._w,X_val[i])<1).astype(int)\n",
        "            if verbose and epoch % 20 == 0:\n",
        "                print('epoch {}. Errors={}. Mean Hinge_loss={}'\\\n",
        "                      .format(epoch,val_err, val_loss))\n",
        "            train_errors.append(tr_err)\n",
        "            val_errors.append(val_err)\n",
        "            train_loss_epoch.append(tr_loss)\n",
        "            val_loss_epoch.append(val_loss)\n",
        "        self.history_w = np.array(self.history_w)    \n",
        "        self.train_errors = np.array(train_errors)\n",
        "        self.val_errors = np.array(val_errors)\n",
        "        self.train_loss = np.array(train_loss_epoch)\n",
        "        self.val_loss = np.array(val_loss_epoch)                    \n",
        "\n",
        "    def predict(self, X:np.array) -> np.array:\n",
        "        y_pred = []\n",
        "        X_extended = add_bias_feature(X)\n",
        "        for i in range(len(X_extended)):\n",
        "            y_pred.append(np.sign(np.dot(self._w,X_extended[i])))\n",
        "        return np.array(y_pred)         \n",
        "\n",
        "    def hinge_loss(self, x, y):\n",
        "        return max(0,1 - y*np.dot(x, self._w))\n",
        "\n",
        "    def soft_margin_loss(self, x, y):\n",
        "        return self.hinge_loss(x,y)+self._alpha*np.dot(self._w, self._w)\n",
        "\n",
        "\n",
        "svm = SVM(epochs=2000)\n",
        "svm.fit(np.vstack([get_conv_numbers(0), get_conv_numbers(1)]), np.array([-1,-1,-1,-1,-1,1,1,1,1,1]),\n",
        "        np.vstack([get_conv_numbers(0)[3:], get_conv_numbers(1)[3:]]), np.array([-1,-1,-1,1,1,1]),\n",
        "        verbose=True)\n",
        "# Предсказать число, если чашка то output = -1, иначе 1\n",
        "number =  np.array([0,1,0,0,0,\n",
        "                    1,0,1,1,0,\n",
        "                    1,1,1,0,1,\n",
        "                    1,0,1,1,0,\n",
        "                    1,1,1,0,0])\n",
        "print(svm.predict(np.array([get_conv_number(number)])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLB7M8cyAJV1",
        "outputId": "e3e63792-f197-46d0-b69f-6bb7177cfbfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0. Errors=4. Mean Hinge_loss=5.359884795263408\n",
            "epoch 20. Errors=1. Mean Hinge_loss=2.5211145219702766\n",
            "epoch 40. Errors=1. Mean Hinge_loss=2.520932539497231\n",
            "epoch 60. Errors=1. Mean Hinge_loss=2.5207505811966437\n",
            "epoch 80. Errors=1. Mean Hinge_loss=2.5205686470649145\n",
            "epoch 100. Errors=1. Mean Hinge_loss=2.5203867370984265\n",
            "epoch 120. Errors=1. Mean Hinge_loss=2.5202048512935673\n",
            "epoch 140. Errors=1. Mean Hinge_loss=2.520022989646722\n",
            "epoch 160. Errors=1. Mean Hinge_loss=2.5198411521542914\n",
            "epoch 180. Errors=1. Mean Hinge_loss=2.51965933881266\n",
            "epoch 200. Errors=1. Mean Hinge_loss=2.5194775496182227\n",
            "epoch 220. Errors=1. Mean Hinge_loss=2.5192957845673662\n",
            "epoch 240. Errors=1. Mean Hinge_loss=2.519114043656492\n",
            "epoch 260. Errors=1. Mean Hinge_loss=2.5189323268819845\n",
            "epoch 280. Errors=1. Mean Hinge_loss=2.51875063424024\n",
            "epoch 300. Errors=1. Mean Hinge_loss=2.518568965727655\n",
            "epoch 320. Errors=1. Mean Hinge_loss=2.5183873213406245\n",
            "epoch 340. Errors=1. Mean Hinge_loss=2.5182057010755425\n",
            "epoch 360. Errors=1. Mean Hinge_loss=2.518024104928805\n",
            "epoch 380. Errors=1. Mean Hinge_loss=2.5178425328968133\n",
            "epoch 400. Errors=1. Mean Hinge_loss=2.5176609849759646\n",
            "epoch 420. Errors=1. Mean Hinge_loss=2.5174794611626514\n",
            "epoch 440. Errors=1. Mean Hinge_loss=2.5172979614532776\n",
            "epoch 460. Errors=1. Mean Hinge_loss=2.517116485844239\n",
            "epoch 480. Errors=1. Mean Hinge_loss=2.5169350343319374\n",
            "epoch 500. Errors=1. Mean Hinge_loss=2.5167536069127707\n",
            "epoch 520. Errors=1. Mean Hinge_loss=2.516572203583145\n",
            "epoch 540. Errors=1. Mean Hinge_loss=2.51639082433946\n",
            "epoch 560. Errors=1. Mean Hinge_loss=2.5162094691781167\n",
            "epoch 580. Errors=1. Mean Hinge_loss=2.5160281380955096\n",
            "epoch 600. Errors=1. Mean Hinge_loss=2.51584683108806\n",
            "epoch 620. Errors=1. Mean Hinge_loss=2.5156655481521653\n",
            "epoch 640. Errors=1. Mean Hinge_loss=2.5154842892842284\n",
            "epoch 660. Errors=1. Mean Hinge_loss=2.515303054480655\n",
            "epoch 680. Errors=1. Mean Hinge_loss=2.51512184373785\n",
            "epoch 700. Errors=1. Mean Hinge_loss=2.5149406570522177\n",
            "epoch 720. Errors=1. Mean Hinge_loss=2.5147594944201703\n",
            "epoch 740. Errors=1. Mean Hinge_loss=2.5145783558381143\n",
            "epoch 760. Errors=1. Mean Hinge_loss=2.514397241302451\n",
            "epoch 780. Errors=1. Mean Hinge_loss=2.514216150809602\n",
            "epoch 800. Errors=1. Mean Hinge_loss=2.5140350843559705\n",
            "epoch 820. Errors=1. Mean Hinge_loss=2.513854041937966\n",
            "epoch 840. Errors=1. Mean Hinge_loss=2.5136730235519984\n",
            "epoch 860. Errors=1. Mean Hinge_loss=2.513492029194481\n",
            "epoch 880. Errors=1. Mean Hinge_loss=2.5133110588618246\n",
            "epoch 900. Errors=1. Mean Hinge_loss=2.5131301125504417\n",
            "epoch 920. Errors=1. Mean Hinge_loss=2.512949190256739\n",
            "epoch 940. Errors=1. Mean Hinge_loss=2.5127682919771415\n",
            "epoch 960. Errors=1. Mean Hinge_loss=2.512587417708061\n",
            "epoch 980. Errors=1. Mean Hinge_loss=2.51240656744591\n",
            "epoch 1000. Errors=1. Mean Hinge_loss=2.512225741187102\n",
            "epoch 1020. Errors=1. Mean Hinge_loss=2.5120449389280557\n",
            "epoch 1040. Errors=1. Mean Hinge_loss=2.5118641606651884\n",
            "epoch 1060. Errors=1. Mean Hinge_loss=2.5116834063949156\n",
            "epoch 1080. Errors=1. Mean Hinge_loss=2.511502676113655\n",
            "epoch 1100. Errors=1. Mean Hinge_loss=2.5113219698178306\n",
            "epoch 1120. Errors=1. Mean Hinge_loss=2.511141287503853\n",
            "epoch 1140. Errors=1. Mean Hinge_loss=2.510960629168144\n",
            "epoch 1160. Errors=1. Mean Hinge_loss=2.510779994807127\n",
            "epoch 1180. Errors=1. Mean Hinge_loss=2.5105993844172234\n",
            "epoch 1200. Errors=1. Mean Hinge_loss=2.5104187979948525\n",
            "epoch 1220. Errors=1. Mean Hinge_loss=2.510238235536434\n",
            "epoch 1240. Errors=1. Mean Hinge_loss=2.5100576970383925\n",
            "epoch 1260. Errors=1. Mean Hinge_loss=2.509877182497152\n",
            "epoch 1280. Errors=1. Mean Hinge_loss=2.509696691909138\n",
            "epoch 1300. Errors=1. Mean Hinge_loss=2.509516225270766\n",
            "epoch 1320. Errors=1. Mean Hinge_loss=2.509335782578469\n",
            "epoch 1340. Errors=1. Mean Hinge_loss=2.5091553638286714\n",
            "epoch 1360. Errors=1. Mean Hinge_loss=2.5089749690177965\n",
            "epoch 1380. Errors=1. Mean Hinge_loss=2.508794598142278\n",
            "epoch 1400. Errors=1. Mean Hinge_loss=2.5086142511985376\n",
            "epoch 1420. Errors=1. Mean Hinge_loss=2.508433928183002\n",
            "epoch 1440. Errors=1. Mean Hinge_loss=2.5082536290921045\n",
            "epoch 1460. Errors=1. Mean Hinge_loss=2.508073353922273\n",
            "epoch 1480. Errors=1. Mean Hinge_loss=2.507893102669932\n",
            "epoch 1500. Errors=1. Mean Hinge_loss=2.5077128753315185\n",
            "epoch 1520. Errors=1. Mean Hinge_loss=2.50753267190346\n",
            "epoch 1540. Errors=1. Mean Hinge_loss=2.5073524923821924\n",
            "epoch 1560. Errors=1. Mean Hinge_loss=2.5071723367641394\n",
            "epoch 1580. Errors=1. Mean Hinge_loss=2.5069922050457385\n",
            "epoch 1600. Errors=1. Mean Hinge_loss=2.506812097223424\n",
            "epoch 1620. Errors=1. Mean Hinge_loss=2.506632013293632\n",
            "epoch 1640. Errors=1. Mean Hinge_loss=2.5064519532527925\n",
            "epoch 1660. Errors=1. Mean Hinge_loss=2.50627191709734\n",
            "epoch 1680. Errors=1. Mean Hinge_loss=2.506091904823713\n",
            "epoch 1700. Errors=1. Mean Hinge_loss=2.5059119164283468\n",
            "epoch 1720. Errors=1. Mean Hinge_loss=2.5057319519076775\n",
            "epoch 1740. Errors=1. Mean Hinge_loss=2.5055520112581404\n",
            "epoch 1760. Errors=1. Mean Hinge_loss=2.505372094476175\n",
            "epoch 1780. Errors=1. Mean Hinge_loss=2.5051922015582258\n",
            "epoch 1800. Errors=1. Mean Hinge_loss=2.5050123325007285\n",
            "epoch 1820. Errors=1. Mean Hinge_loss=2.5048324873001193\n",
            "epoch 1840. Errors=1. Mean Hinge_loss=2.504652665952839\n",
            "epoch 1860. Errors=1. Mean Hinge_loss=2.504472868455331\n",
            "epoch 1880. Errors=1. Mean Hinge_loss=2.5042930948040394\n",
            "epoch 1900. Errors=1. Mean Hinge_loss=2.5041133449953996\n",
            "epoch 1920. Errors=1. Mean Hinge_loss=2.5039336190258568\n",
            "epoch 1940. Errors=1. Mean Hinge_loss=2.5037539168918563\n",
            "epoch 1960. Errors=1. Mean Hinge_loss=2.503574238589847\n",
            "epoch 1980. Errors=1. Mean Hinge_loss=2.503394584116261\n",
            "[-1.]\n"
          ]
        }
      ]
    }
  ]
}